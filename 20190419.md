# Foundations: How to design experiments in NLU


**Author** Sam Bowman
- Assistant Professor at NYU

**Source** [Slides](https://drive.google.com/file/d/1kUarnxZNa-ojz0KvZFmHeAEFwTkEH7lg/edit)

**Summary**

- **Reading Related Work**
  - **Why?** By reading them, you can
    - avoid repeating work
    - learn about common methods, datasets, and libraries
    - think what questions haven't been answered
  - **How to find papers?** Keyword search on [**Google Scholar**](https://scholar.google.com) or the [**ACL Anthology**](https://www.aclweb.org/anthology/) (only for NLP).
  - **How to filter papers?**
    - Identify papers that are 1. relevant 2. appear often 3. have lots of citation.
    - Most useful papers are **recent papers that cite older papers** that you are also interested in! (Might contain good summary of old works!)
    - Papers published in **top conferences** are more trustworthy than arXiv or published elsewhere.
    - Published papers with **negative results**, as they are held to a higher standard in publishing.
- **Organizing a Project & Doing Good Science**
    - Set a informative, falsifiable **hypothesis**
    - **Baselines** are:
      - simplest reasonable approaches to the problem
      - existing work
      - best published number for the problem (state of the art)
    - Knowing the human performance (upper bound) can make your accuracy more convincing
    - Use at least one dataset that appeared in related work, even if the specific problem is different
      - Find them in [ACL anthology](https://aclweb.org/anthology/) or [Linguistic Data Consortium](https://www.ldc.upenn.edu/)
      - Build them. (Make sure to write in detail how and why to convince the reviewers)
      - Scrape them? (Be careful not to break the law.)
      - Have someone else build them. (Often quick and cheap. ex. MTurk)
         - Try it yourself to make sure task and instructions are easy.
         - Communicate with your annotators.
    - Artificial generated datasets are great for **preliminary experiments** to validate your idea and analyze errors. It however won't convince your reviewers.
    - In **Quantitative Evaluation**, follow prior work precisely for your main evaluation metric.
      - Do as much ablation analysis as possible with as many variants.
      - Use human evaluations for tasks if it is standard in the field.
      - Invent new analysis metrics if they augment your point.
      - If the output of your system can be an input of another system, perform extrinsic evaluations of those tasks.
      - Test for **statistical significance**. ([Berg-Kirkpatrick et al. '12](http://nlp.cs.berkeley.edu/pubs/BergKirkpatrick-Burkett-Klein_2012_Significance_paper.pdf))
        - "a little significant" (p < 0.1)
        - "teethering in the brink of significance" (p = 0.06)
        - "loosely significant" (p = 0.10)
      - If your results are below baseline or insignificant (Negative results), indicate it and explain what you found.
    - In **Qualitative Evaluation**, convince the reader of your hypothesis.
      - Check prior work to see their qualitative analysis.
      - **Formative evaluation** Compare design options. Tune hyperparameters.
      - **Summative evaluation** Compare with major variants of your approach and with prior works with the **test set**.
    - **Hyperparameter tuning**
      - **Tune your baseline as thoroughly as you tune your own model.**
      - Read previous works to understand sensitive hyperparameters and normal values for hyperparameters.
      - **How to tune?**
        - Bayesian optimization is optimal but might be hard to use.
        - Random search is easy and near optimal.  ([Bergstra and Bengio '12](http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf)).
          - Define distributions over all your hyperparameters.
          - Sample N times for N experiments.
          - Look for patterns in your results.
          - Adjust the distributions and repeat until you run out of resources or
        performance stops improving.
  - Iterative development
    - As soon as you start:
       - Create a git repo
       - Find or write code to load data
       - Find code to evaluate results (Try not to write)
       - Find or write a **very simple** baseline
    - Save model checkpoint files for all reasonably interesting experiments.
    - Note what each experiment was meant to test.
  - Recommended Reading
    - [Resnik and Lin on evaluation](http://www.cs.colorado.edu/~jbg/teaching/CMSC_773_2012/reading/evaluation.pdf)
    - [Garnder slides on engineering](http://www.cs.colorado.edu/~jbg/teaching/CMSC_773_2012/reading/evaluation.pdf)
    - [Advice for Research Students](https://www.cs.jhu.edu/~jason/advice/)


**Discussion**

- Most of these suggestions are for general ML/AI researchers. Awesome!
- The significance phrases in page 31 is great! Not sure how reliable it is though.
- Confused about convincing the reader of your hypothesis for qualitative evaluation. Shouldn't hypothesis be falsifiable and thus objective?
