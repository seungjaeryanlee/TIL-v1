# Deep RL Bootcamp Lecture 9 Model-based Reinforcement Learning

**THIS IS A WORK IN PROGRESS**


**Speaker** Chelsea Finn
- Ph.D student at UC Berkeley

**Source** [YouTube](https://www.youtube.com/watch?v=iC2a7M9voYU)

### Summary

- The lecture will follow this outline:
  - Why use model-based RL?
  - Summary of main model-based RL approaches
  - Using local models & guided policy search
  - Handling high-dimensional observations in model-based RL

- Why use model-based RL?
  - The primary reason for using model-based RL is for **sample efficiency.**
    - **Gradient-free methods** (NES, CMA, etc...)
    - **Fully online methods** (A3C) requires 10x less samples than gradient free methods.
    - **Policy gradient methods** (TRPO) requires 10x less samples than fully online methods.
    - **Replay buffer value estimation methods** (Q-learning, DDPG, NAF, etc...) requires 10x less samples than policy gradient methods.
    - **Model-based Deep RL** (Guided policy search) requires 10x less samples than replay buffer value estimation methods.
    - **Model-based "shallow" RL** (PILCO) requires 10x less samples than model-based deep RL methods.
  - Model-based RL is generally **more transferrable and general**, since a model can be reused for achieving different tasks.

- Main model-based RL approaches
  - General RL algorithm
    1. Generate Samples with policy
    2. Fit model to estimate return
    3. Improve policy
    4. Repeat
  - Model-based Approach
    - Use supervised learning to estimate the model (2)
    - Different ways to represent model
    - Different ways to improve policy
    1. Backpropagation
       1. Run base policy to collect experience
       2. Learn model
       3. Backpropagate through model into policy to improve policy
    

### Discussion

- She speaks quite fast! I thought I had in on 1.25 speed the entire time.